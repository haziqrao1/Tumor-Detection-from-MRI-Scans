# -*- coding: utf-8 -*-
"""DIP_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EKhXUpFTWm1arOq8Qm1YUQnOx-MRQZl-
"""

from google.colab import drive
drive.mount('/content/drive')

"""Muhammad Haziq Rao

Afaq Asif
"""

import numpy as np
import cv2
import os
import pickle
import glob
from sklearn.utils import shuffle 
from sklearn import svm,metrics
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import log_loss
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

dsize = (224,224)

img_dir = "/content/drive/MyDrive/brain_tumor/no"
data_path = os.path.join(img_dir,'*') 
no_images = glob.glob(data_path) 
data_no = [] 
for i in no_images: 
    img = cv2.imread(i)
    img = cv2.resize(img,dsize) 
    img = img / 255.
    data_no.append(img)

len(data_no)

data_no[0].shape

img_dir = "/content/drive/MyDrive/brain_tumor/yes/"
data_path = os.path.join(img_dir,'*') 
no_images = glob.glob(data_path) 
data_yes = []
k=0 
for i in no_images: 
    img = cv2.imread(i)
    img = cv2.resize(img,dsize) 
    img = img / 255.
    data_yes.append(img)

len(data_yes)

data_yes[0].shape

X = []
y = []
for i in data_yes:
  X.append(i)
  y.append(1)

for i in data_no:
  X.append(i)
  y.append(0)

len(X)

X = np.array(X)
y = np.array(y)
y.shape

X,y = shuffle(X,y)
y.shape

X_train,X_test ,y_train, y_test =  train_test_split(X, y, test_size=0.2)
y_train.shape

X_test.shape

X_train.shape

X_train.shape
y_train.shape

X_train = X_train.reshape(X_train.shape[0], 150528)
y_train = y_train.reshape(y_train.shape[0], )

# Create a classifier: a support vector classifier
param_grid = [
  {'C': [1, 10,20,30,40], 'kernel': ['linear']},
 ]
svc = svm.SVC()
classifier = GridSearchCV(svc, param_grid, verbose = 3)
classifier.fit(X_train, y_train)
#fit to the training data

X_test = X_test.reshape(X_test.shape[0], 150528)
y_test = y_test.reshape(y_test.shape[0], )

y_pred = classifier.predict(X_test)

# Commented out IPython magic to ensure Python compatibility.
print("Classification report for classifier %s:\n%s\n"
#       % (classifier, metrics.classification_report(y_test, y_pred)))

knn = KNeighborsClassifier(n_neighbors = 1)
knn.fit(X_train, y_train)
knn.score(X_test, y_test)

i = 1
neighbours = []
while i<176:    #n samples-1
  knn = KNeighborsClassifier(n_neighbors = i)
  knn.fit(X_train, y_train)
  score = knn.score(X_test, y_test)
  neighbours.append(score) 
  i = i+1

max_value = max(neighbours) #max score value when neighbours = 97
max_index = neighbours.index(max_value)

knn_simple = KNeighborsClassifier(n_neighbors = max_index + 1) # as we take "i" as 1 in first iteration
knn_simple.fit(X_train, y_train)
knn_simple.score(X_test, y_test)

hog = cv2.HOGDescriptor()
 im = cv2.imread('/content/drive/MyDrive/brain_tumor/no/1 no.jpeg')
 print(im.shape)
 h = hog.compute(im)

h.shape

from skimage.feature import hog
import matplotlib.pyplot as plt

fd, hog_image = hog(im, orientations=9, pixels_per_cell=(8, 8),
                	cells_per_block=(2, 2), visualize=True, multichannel=True)
plt.axis("off")
plt.imshow(hog_image, cmap="gray")

fd.shape

hog_image.shape

img_dir = "/content/drive/MyDrive/brain_tumor/no"
data_path = os.path.join(img_dir,'*') 
no_images = glob.glob(data_path) 
data_no_feature = []
data_no_hog_image = [] 
for i in no_images: 
    img = cv2.imread(i)
    img = cv2.resize(img,dsize) 
    img = img / 255.
    fd, hog_image = hog(img, orientations=9, pixels_per_cell=(8, 8),
                	cells_per_block=(2, 2), visualize=True, multichannel=True)
    data_no_feature.append(fd)
    data_no_hog_image.append(hog_image)

img_dir = "/content/drive/MyDrive/brain_tumor/yes"
data_path = os.path.join(img_dir,'*') 
no_images = glob.glob(data_path) 
data_yes_feature = []
data_yes_hog_image = [] 
for i in no_images: 
    img = cv2.imread(i)
    img = cv2.resize(img,dsize) 
    img = img / 255.
    fd, hog_image = hog(img, orientations=9, pixels_per_cell=(8, 8),
                	cells_per_block=(2, 2), visualize=True, multichannel=True)
    data_yes_feature.append(fd)
    data_yes_hog_image.append(hog_image)

X_hog = []
y_hog = []
for i in data_yes_feature:
  X_hog.append(i)
  y_hog.append(1)

for i in data_no_feature:
  X_hog.append(i)
  y_hog.append(0)

X_hog = np.array(X_hog)
y_hog = np.array(y_hog)

X_hog,y_hog = shuffle(X_hog,y_hog)

X_hog_train,X_hog_test ,y_hog_train, y_hog_test =  train_test_split(X_hog, y_hog, test_size=0.2)
y_hog_train.shape

X_hog_train.shape

# Create a classifier: a support vector classifier
param_grid = [
  {'C': [1, 10,20,30,40,50,60,70,80], 'kernel': ['linear']},
 ]
svc = svm.SVC()
classifier = GridSearchCV(svc, param_grid, verbose = 100)
classifier.fit(X_hog_train, y_hog_train)
#fit to the training data

y_pred = classifier.predict(X_hog_test)

# Commented out IPython magic to ensure Python compatibility.
print("Classification report for classifier %s:\n%s\n"
#       % (classifier, metrics.classification_report(y_hog_test, y_pred)))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn import metrics
sns.set()
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

y_pred = classifier.predict(X_hog_test)
cnf_matrix = metrics.confusion_matrix(y_hog_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

knn = KNeighborsClassifier(n_neighbors = 1)
knn.fit(X_hog_train, y_hog_train)
knn.score(X_hog_test, y_hog_test)

i = 1
neighbours = []
while i<176:    #n samples-1
  knn = KNeighborsClassifier(n_neighbors = i)
  knn.fit(X_hog_train, y_hog_train)
  score = knn.score(X_hog_test, y_hog_test)
  neighbours.append(score) 
  i = i+1

max_value = max(neighbours) #max score value when neighbours = 97
max_index = neighbours.index(max_value)

knn = KNeighborsClassifier(n_neighbors = max_index + 1) # as we take "i" as 1 in first iteration
knn.fit(X_hog_train, y_hog_train)
knn.score(X_hog_test, y_hog_test)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn import metrics
sns.set()
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

y_pred = knn.predict(X_hog_test)
cnf_matrix = metrics.confusion_matrix(y_hog_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

# Making the final model using voting classifier
final_model = VotingClassifier(
    estimators=[('KNN_Simple',knn), ('SVM_HOG', classifier)], voting='hard')
 
# training all the model on the train dataset
final_model.fit(X_hog_train, y_hog_train)
 
# predicting the output on the test dataset
pred_final = final_model.predict(X_hog_test)
 
# printing log loss between actual and predicted value
print(log_loss(y_hog_test, pred_final))

# Commented out IPython magic to ensure Python compatibility.
print("Classification report for classifier %s:\n%s\n"
#       % (final_model, metrics.classification_report(y_hog_test, pred_final)))

# save the model to disk
filename = '/content/drive/MyDrive/brain_tumor/final_model.h5'
pickle.dump(final_model, open(filename, 'wb'))
#final_model.save('/content/drive/MyDrive/brain_tumor/final_model.h5')

# load the model from disk
filename = '/content/drive/MyDrive/brain_tumor/final_model.h5'
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_hog_train, y_hog_train)
print(result)
#my_model = load_model('/content/drive/MyDrive/brain_tumor/final_model.h5')

from sklearn import tree

classifier_decision_tree = tree.DecisionTreeClassifier()
classifier_decision_tree.fit(X_hog_train,y_hog_train)

tree.plot_tree(classifier_decision_tree)

classifier_decision_tree.score(X_hog_test, y_hog_test)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn import metrics
sns.set()
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

y_pred = classifier_decision_tree.predict(X_hog_test)
cnf_matrix = metrics.confusion_matrix(y_hog_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

from sklearn.linear_model import SGDClassifier

sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)
sgd.fit(X_hog_train, y_hog_train)
sgd.score(X_hog_test,y_hog_test)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn import metrics
sns.set()
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

y_pred = sgd.predict(X_hog_test)
cnf_matrix = metrics.confusion_matrix(y_hog_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')



"""Added 3 clssifier

"""

# Making the final model using voting classifier
final_model = VotingClassifier(
    estimators=[('KNN_Simple',knn), ('SVM_HOG', classifier),('SGD_HOG',sgd)], voting='hard')
 
# training all the model on the train dataset
final_model.fit(X_hog_train, y_hog_train)
 
# predicting the output on the test dataset
pred_final = final_model.predict(X_hog_test)
 
# printing log loss between actual and predicted value
print(log_loss(y_hog_test, pred_final))

# Commented out IPython magic to ensure Python compatibility.
print("Classification report for classifier %s:\n%s\n"
#       % (final_model, metrics.classification_report(y_hog_test, pred_final)))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn import metrics
sns.set()
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

y_pred = final_model.predict(X_hog_test)
cnf_matrix = metrics.confusion_matrix(y_hog_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

# Create a classifier: a support vector classifier
param_grid = [
  {'C': [1, 10,20,30,40,50,60,70,80], 'kernel': ['linear']},
 ]
svc = svm.SVC()
classifier = GridSearchCV(svc, param_grid, verbose = 100)
classifier.fit(X_hog_train, y_hog_train)
#fit to the training data

# Commented out IPython magic to ensure Python compatibility.
y_pred = classifier.predict(X_hog_test)
print("Classification report for classifier %s:\n%s\n"
#       % (classifier, metrics.classification_report(y_hog_test, y_pred)))

knn = KNeighborsClassifier(n_neighbors = 1)
knn.fit(X_hog_train, y_hog_train)
knn.score(X_hog_test, y_hog_test)

i = 1
neighbours = []
while i<176:    #n samples-1
  knn = KNeighborsClassifier(n_neighbors = i)
  knn.fit(X_hog_train, y_hog_train)
  score = knn.score(X_hog_test, y_hog_test)
  neighbours.append(score) 
  i = i+1

max_value = max(neighbours) #max score value when neighbours = 97
max_index = neighbours.index(max_value)

knn = KNeighborsClassifier(n_neighbors = max_index + 1) # as we take "i" as 1 in first iteration
knn.fit(X_hog_train, y_hog_train)
knn.score(X_hog_test, y_hog_test)

